{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff5af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoModel\n",
    "import sys\n",
    "import math\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as perf\n",
    "from scipy.stats import pearsonr\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from utils.preprocessing import load_data\n",
    "from utils.transformer_dataset import ReviewDataset\n",
    "from utils.training import train_text_model, train_text_meta_model, train_meta_model\n",
    "from utils.evaluation import get_cls_perf, get_reg_perf, get_predictions\n",
    "from models.transformer_models import UsefulScoreRegressorTextOnly, UsefulScoreRegressorAllFeat, UsefulScoreRegressorMetaOnly\n",
    "from models.transformer_models import UsefulScoreRegressorLinearBaseline, DrugLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f9f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonTextCols = ['ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
    "               'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss', 'ratingNormalized']\n",
    "targetCol = 'usefulCountClass'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d4b80",
   "metadata": {},
   "source": [
    "# No usefulCount cap and no year filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595c992",
   "metadata": {},
   "source": [
    "# usefulCount cap of 99th percentile and years 2009 to 2013 filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3f33d",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d793f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.07397957, train loss: 0.07862058\n",
      "Epoch 1, val loss: 0.07397957 -> 0.07244341, train loss: 0.07569571\n",
      "Epoch 2, val loss: 0.07464819, train loss: 0.07410152\n",
      "Epoch 3, val loss: 0.07353341, train loss: 0.07354082\n",
      "Epoch 4, val loss: 0.07244341 -> 0.07212694, train loss: 0.07356331\n",
      "Epoch 5, val loss: 0.07239426, train loss: 0.07328345\n",
      "Epoch 6, val loss: 0.07257499, train loss: 0.07250335\n",
      "Epoch 7, val loss: 0.07244378, train loss: 0.07279101\n",
      "Epoch 8, val loss: 0.07227632, train loss: 0.07209536\n",
      "Epoch 9, val loss: 0.07458339, train loss: 0.07204516\n",
      "f1: 0.7084690178343485, acc: 0.7093584177520502\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.12647324, train loss: 0.12629476\n",
      "Epoch 1, val loss: 0.12647324 -> 0.12386728, train loss: 0.12218056\n",
      "Epoch 2, val loss: 0.12386728 -> 0.11931118, train loss: 0.12074414\n",
      "Epoch 3, val loss: 0.11931118 -> 0.11825148, train loss: 0.12030316\n",
      "Epoch 4, val loss: 0.12066339, train loss: 0.11919602\n",
      "Epoch 5, val loss: 0.12062639, train loss: 0.11900482\n",
      "Epoch 6, val loss: 0.11825148 -> 0.11792949, train loss: 0.11941074\n",
      "Epoch 7, val loss: 0.12064110, train loss: 0.11909745\n",
      "Epoch 8, val loss: 0.11959275, train loss: 0.11860086\n",
      "Epoch 9, val loss: 0.12241057, train loss: 0.11811673\n",
      "f1: 0.55352996697932, acc: 0.5487216594307767\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.15562239, train loss: 0.16038392\n",
      "Epoch 1, val loss: 0.15562239 -> 0.15348731, train loss: 0.15521719\n",
      "Epoch 2, val loss: 0.15846869, train loss: 0.15481228\n",
      "Epoch 3, val loss: 0.15348731 -> 0.15336014, train loss: 0.15331127\n",
      "Epoch 4, val loss: 0.15418938, train loss: 0.15333028\n",
      "Epoch 5, val loss: 0.15427197, train loss: 0.15270639\n",
      "Epoch 6, val loss: 0.15520688, train loss: 0.15295736\n",
      "Epoch 7, val loss: 0.15336014 -> 0.15173476, train loss: 0.15204734\n",
      "Epoch 8, val loss: 0.15173476 -> 0.15163752, train loss: 0.15141019\n",
      "Epoch 9, val loss: 0.15296284, train loss: 0.15108612\n",
      "f1: 0.39644252546605613, acc: 0.4339122045344911\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.18329719, train loss: 0.18750482\n",
      "Epoch 1, val loss: 0.18979755, train loss: 0.18342581\n",
      "Epoch 2, val loss: 0.18329719 -> 0.17997063, train loss: 0.18162689\n",
      "Epoch 3, val loss: 0.18400246, train loss: 0.18130777\n",
      "Epoch 4, val loss: 0.18065880, train loss: 0.18053920\n",
      "Epoch 5, val loss: 0.18199984, train loss: 0.18020246\n",
      "Epoch 6, val loss: 0.18007204, train loss: 0.18018453\n",
      "Epoch 7, val loss: 0.18137919, train loss: 0.18021469\n",
      "Epoch 8, val loss: 0.18195506, train loss: 0.17881925\n",
      "Epoch 9, val loss: 0.18371830, train loss: 0.17943231\n",
      "f1: 0.31877016609980846, acc: 0.3791606367583213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2009, 2013], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    ##### Text-only Transformer Model\n",
    "    encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "    # Freeze encoder parameters to avoid CUDA out of memory.\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = UsefulScoreRegressorTextOnly(encoder, outputs=num_vals)  # classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_text_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                     train_loader=train_loader, val_loader=val_loader,\n",
    "                     criterion=criterion,\n",
    "                     save_path=f'../models/Classifiers/distilBERT_Frozen_TextOnly_Classify{num_vals}.pt',\n",
    "                     clip=1.0,\n",
    "                     classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/distilBERT_Frozen_TextOnly_Classify{num_vals}.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da22258",
   "metadata": {},
   "source": [
    "#### Text-Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a042c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.07894914, train loss: 0.07412356\n",
      "Epoch 1, val loss: 0.07894914 -> 0.07483272, train loss: 0.07099397\n",
      "Epoch 2, val loss: 0.07483272 -> 0.07315473, train loss: 0.07054865\n",
      "Epoch 3, val loss: 0.07315473 -> 0.07162509, train loss: 0.06930481\n",
      "Epoch 4, val loss: 0.07162509 -> 0.07013874, train loss: 0.06984539\n",
      "Epoch 5, val loss: 0.08012029, train loss: 0.06891072\n",
      "Epoch 6, val loss: 0.07013874 -> 0.06934771, train loss: 0.06856231\n",
      "Epoch 7, val loss: 0.07182907, train loss: 0.06897483\n",
      "Epoch 8, val loss: 0.07228963, train loss: 0.06800907\n",
      "Epoch 9, val loss: 0.07055551, train loss: 0.06779471\n",
      "f1: 0.7241471284132024, acc: 0.7257597684515196\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.11995788, train loss: 0.12187148\n",
      "Epoch 1, val loss: 0.11995788 -> 0.11339937, train loss: 0.11671966\n",
      "Epoch 2, val loss: 0.11775153, train loss: 0.11595232\n",
      "Epoch 3, val loss: 0.11472886, train loss: 0.11498868\n",
      "Epoch 4, val loss: 0.11993646, train loss: 0.11457653\n",
      "Epoch 5, val loss: 0.11339937 -> 0.11240951, train loss: 0.11391138\n",
      "Epoch 6, val loss: 0.11562744, train loss: 0.11394991\n",
      "Epoch 7, val loss: 0.11792131, train loss: 0.11387904\n",
      "Epoch 8, val loss: 0.11668270, train loss: 0.11405753\n",
      "Epoch 9, val loss: 0.11559086, train loss: 0.11260378\n",
      "f1: 0.5806216338110883, acc: 0.58803666184274\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.14965397, train loss: 0.15494812\n",
      "Epoch 1, val loss: 0.14965397 -> 0.14812401, train loss: 0.14949239\n",
      "Epoch 2, val loss: 0.14812401 -> 0.14752596, train loss: 0.14777927\n",
      "Epoch 3, val loss: 0.14752596 -> 0.14702433, train loss: 0.14723278\n",
      "Epoch 4, val loss: 0.14712343, train loss: 0.14650919\n",
      "Epoch 5, val loss: 0.14722407, train loss: 0.14611327\n",
      "Epoch 6, val loss: 0.14702433 -> 0.14569425, train loss: 0.14681260\n",
      "Epoch 7, val loss: 0.14824853, train loss: 0.14539513\n",
      "Epoch 8, val loss: 0.14974093, train loss: 0.14493197\n",
      "Epoch 9, val loss: 0.14856823, train loss: 0.14463536\n",
      "f1: 0.4558750500635986, acc: 0.47081524360829713\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.17917263, train loss: 0.18179161\n",
      "Epoch 1, val loss: 0.17917263 -> 0.17722115, train loss: 0.17639215\n",
      "Epoch 2, val loss: 0.17722115 -> 0.17414836, train loss: 0.17524154\n",
      "Epoch 3, val loss: 0.17549802, train loss: 0.17421447\n",
      "Epoch 4, val loss: 0.17414836 -> 0.17370326, train loss: 0.17412958\n",
      "Epoch 5, val loss: 0.17706589, train loss: 0.17301640\n",
      "Epoch 6, val loss: 0.17469562, train loss: 0.17277673\n",
      "Epoch 7, val loss: 0.17370326 -> 0.17328874, train loss: 0.17240503\n",
      "Epoch 8, val loss: 0.17896442, train loss: 0.17202140\n",
      "Epoch 9, val loss: 0.17886223, train loss: 0.17137479\n",
      "f1: 0.33441931811374664, acc: 0.3958031837916064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2009, 2013], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "    # Freeze encoder parameters to avoid CUDA out of memory.\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = UsefulScoreRegressorAllFeat(encoder, num_meta_feats=len(nonTextCols), outputs=num_vals)  # classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_text_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                          train_loader=train_loader, val_loader=val_loader,\n",
    "                          criterion=criterion,\n",
    "                          save_path=f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}.pt',\n",
    "                          clip=1.0,\n",
    "                          classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT-META')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00212e0",
   "metadata": {},
   "source": [
    "#### Neural Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ac8e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.07052004, train loss: 0.07207060\n",
      "Epoch 1, val loss: 0.07052004 -> 0.07033605, train loss: 0.06990190\n",
      "Epoch 2, val loss: 0.07069781, train loss: 0.06976134\n",
      "Epoch 3, val loss: 0.07033605 -> 0.07023999, train loss: 0.06962311\n",
      "Epoch 4, val loss: 0.07023999 -> 0.07021992, train loss: 0.06960955\n",
      "Epoch 5, val loss: 0.07022868, train loss: 0.06945290\n",
      "Epoch 6, val loss: 0.07021992 -> 0.07010770, train loss: 0.06937224\n",
      "Epoch 7, val loss: 0.07018366, train loss: 0.06951573\n",
      "Epoch 8, val loss: 0.07010770 -> 0.07000520, train loss: 0.06945575\n",
      "Epoch 9, val loss: 0.07015875, train loss: 0.06935509\n",
      "f1: 0.7180882641386074, acc: 0.7240713941148095\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.11621064, train loss: 0.12217752\n",
      "Epoch 1, val loss: 0.11621064 -> 0.11573562, train loss: 0.11665317\n",
      "Epoch 2, val loss: 0.11605093, train loss: 0.11618662\n",
      "Epoch 3, val loss: 0.11573562 -> 0.11529325, train loss: 0.11612228\n",
      "Epoch 4, val loss: 0.11529325 -> 0.11522854, train loss: 0.11582194\n",
      "Epoch 5, val loss: 0.11526923, train loss: 0.11585897\n",
      "Epoch 6, val loss: 0.11522854 -> 0.11519532, train loss: 0.11575816\n",
      "Epoch 7, val loss: 0.11525842, train loss: 0.11544410\n",
      "Epoch 8, val loss: 0.11524299, train loss: 0.11547237\n",
      "Epoch 9, val loss: 0.11519532 -> 0.11507961, train loss: 0.11552458\n",
      "f1: 0.5662862314513024, acc: 0.5701881331403763\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.15051000, train loss: 0.15692528\n",
      "Epoch 1, val loss: 0.15051000 -> 0.14979828, train loss: 0.15012806\n",
      "Epoch 2, val loss: 0.14979828 -> 0.14950043, train loss: 0.14928409\n",
      "Epoch 3, val loss: 0.14950043 -> 0.14930727, train loss: 0.14900997\n",
      "Epoch 4, val loss: 0.14937600, train loss: 0.14870765\n",
      "Epoch 5, val loss: 0.14930727 -> 0.14909203, train loss: 0.14869126\n",
      "Epoch 6, val loss: 0.14909203 -> 0.14896609, train loss: 0.14859520\n",
      "Epoch 7, val loss: 0.14908025, train loss: 0.14801536\n",
      "Epoch 8, val loss: 0.14906016, train loss: 0.14802056\n",
      "Epoch 9, val loss: 0.14896609 -> 0.14893163, train loss: 0.14835315\n",
      "f1: 0.4401732209022009, acc: 0.46526772793053545\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.17843647, train loss: 0.18647259\n",
      "Epoch 1, val loss: 0.17843647 -> 0.17716672, train loss: 0.17732596\n",
      "Epoch 2, val loss: 0.17716672 -> 0.17685632, train loss: 0.17663052\n",
      "Epoch 3, val loss: 0.17685632 -> 0.17656053, train loss: 0.17640285\n",
      "Epoch 4, val loss: 0.17656053 -> 0.17633520, train loss: 0.17635338\n",
      "Epoch 5, val loss: 0.17633520 -> 0.17624543, train loss: 0.17582585\n",
      "Epoch 6, val loss: 0.17624543 -> 0.17612801, train loss: 0.17597749\n",
      "Epoch 7, val loss: 0.17643606, train loss: 0.17569619\n",
      "Epoch 8, val loss: 0.17614919, train loss: 0.17546150\n",
      "Epoch 9, val loss: 0.17612801 -> 0.17606180, train loss: 0.17552457\n",
      "f1: 0.32964193206641135, acc: 0.39725036179450074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2009, 2013], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'roberta-base', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'roberta-base', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = UsefulScoreRegressorMetaOnly(num_meta_feats=len(nonTextCols), outputs=num_vals)  #classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                     train_loader=train_loader, val_loader=val_loader,\n",
    "                     criterion=criterion, save_path=f'../models/Classifiers/MetaOnly_NNClassifierBaseline{num_vals}.pt', clip=10000.0,\n",
    "                     classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/MetaOnly_NNClassifierBaseline{num_vals}.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='META')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257da775",
   "metadata": {},
   "source": [
    "#### Linear Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97198c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.07131396, train loss: 0.07458621\n",
      "Epoch 1, val loss: 0.07131396 -> 0.07051269, train loss: 0.06978545\n",
      "Epoch 2, val loss: 0.07051269 -> 0.07040187, train loss: 0.06936264\n",
      "Epoch 3, val loss: 0.07040187 -> 0.07034733, train loss: 0.06928108\n",
      "Epoch 4, val loss: 0.07034733 -> 0.07028681, train loss: 0.06922653\n",
      "Epoch 5, val loss: 0.07028681 -> 0.07028193, train loss: 0.06918832\n",
      "Epoch 6, val loss: 0.07028791, train loss: 0.06915965\n",
      "Epoch 7, val loss: 0.07031276, train loss: 0.06917242\n",
      "Epoch 8, val loss: 0.07028193 -> 0.07026925, train loss: 0.06916486\n",
      "Epoch 9, val loss: 0.07029370, train loss: 0.06915908\n",
      "f1: 0.7189442112730411, acc: 0.7243125904486252\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.11900895, train loss: 0.12557110\n",
      "Epoch 1, val loss: 0.11900895 -> 0.11664075, train loss: 0.11730793\n",
      "Epoch 2, val loss: 0.11664075 -> 0.11610938, train loss: 0.11605261\n",
      "Epoch 3, val loss: 0.11610938 -> 0.11590185, train loss: 0.11564947\n",
      "Epoch 4, val loss: 0.11590185 -> 0.11572416, train loss: 0.11548662\n",
      "Epoch 5, val loss: 0.11572416 -> 0.11565953, train loss: 0.11535283\n",
      "Epoch 6, val loss: 0.11565953 -> 0.11560272, train loss: 0.11525156\n",
      "Epoch 7, val loss: 0.11560272 -> 0.11548736, train loss: 0.11521767\n",
      "Epoch 8, val loss: 0.11548736 -> 0.11545989, train loss: 0.11514328\n",
      "Epoch 9, val loss: 0.11545989 -> 0.11538316, train loss: 0.11512379\n",
      "f1: 0.5602546392587925, acc: 0.5675349734684033\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.15436999, train loss: 0.16176224\n",
      "Epoch 1, val loss: 0.15436999 -> 0.15077666, train loss: 0.15094656\n",
      "Epoch 2, val loss: 0.15077666 -> 0.14990958, train loss: 0.14893298\n",
      "Epoch 3, val loss: 0.14990958 -> 0.14962220, train loss: 0.14834220\n",
      "Epoch 4, val loss: 0.14962220 -> 0.14951047, train loss: 0.14811896\n",
      "Epoch 5, val loss: 0.14951047 -> 0.14939434, train loss: 0.14796760\n",
      "Epoch 6, val loss: 0.14939434 -> 0.14938857, train loss: 0.14785599\n",
      "Epoch 7, val loss: 0.14938857 -> 0.14918286, train loss: 0.14779984\n",
      "Epoch 8, val loss: 0.14918286 -> 0.14916898, train loss: 0.14772097\n",
      "Epoch 9, val loss: 0.14916898 -> 0.14906734, train loss: 0.14765093\n",
      "f1: 0.4335050980677665, acc: 0.4592378195851423\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.18256414, train loss: 0.18872958\n",
      "Epoch 1, val loss: 0.18256414 -> 0.17874049, train loss: 0.17898802\n",
      "Epoch 2, val loss: 0.17874049 -> 0.17772423, train loss: 0.17671825\n",
      "Epoch 3, val loss: 0.17772423 -> 0.17733473, train loss: 0.17600048\n",
      "Epoch 4, val loss: 0.17733473 -> 0.17718949, train loss: 0.17559949\n",
      "Epoch 5, val loss: 0.17718949 -> 0.17698422, train loss: 0.17541350\n",
      "Epoch 6, val loss: 0.17698422 -> 0.17692909, train loss: 0.17524176\n",
      "Epoch 7, val loss: 0.17692909 -> 0.17674701, train loss: 0.17511775\n",
      "Epoch 8, val loss: 0.17674701 -> 0.17661244, train loss: 0.17503556\n",
      "Epoch 9, val loss: 0.17661244 -> 0.17654041, train loss: 0.17495843\n",
      "f1: 0.3239079237830075, acc: 0.3888084901109503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2009, 2013], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'roberta-base', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'roberta-base', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = UsefulScoreRegressorLinearBaseline(num_meta_feats=len(nonTextCols), outputs=num_vals)  #classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                     train_loader=train_loader, val_loader=val_loader,\n",
    "                     criterion=criterion, save_path=f'../models/Classifiers/MetaOnly_LinearClassifierBaseline{num_vals}.pt', clip=10000.0,\n",
    "                     classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/MetaOnly_LinearClassifierBaseline{num_vals}.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='META')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b6d1e",
   "metadata": {},
   "source": [
    "# usefulCount cap of 99th percentile and years 2013 to 2017 with Age Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105dc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonTextCols = ['ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
    "               'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss', 'ratingNormalized', 'ageScore']\n",
    "targetCol = 'usefulCountClass'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02bdb2",
   "metadata": {},
   "source": [
    "#### Text-Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef891a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.05181526, train loss: 0.06054873\n",
      "Epoch 1, val loss: 0.05279917, train loss: 0.05560468\n",
      "Epoch 2, val loss: 0.05197458, train loss: 0.05457137\n",
      "Epoch 3, val loss: 0.05181526 -> 0.05133883, train loss: 0.05420045\n",
      "Epoch 4, val loss: 0.05414853, train loss: 0.05338410\n",
      "Epoch 5, val loss: 0.05154473, train loss: 0.05308257\n",
      "Epoch 6, val loss: 0.05201705, train loss: 0.05276312\n",
      "Epoch 7, val loss: 0.05191622, train loss: 0.05284573\n",
      "Epoch 8, val loss: 0.05620904, train loss: 0.05252063\n",
      "Epoch 9, val loss: 0.05133883 -> 0.05133720, train loss: 0.05197617\n",
      "f1: 0.8136900438206434, acc: 0.8136928539724811\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.09994811, train loss: 0.10484060\n",
      "Epoch 1, val loss: 0.09994811 -> 0.09400103, train loss: 0.09878587\n",
      "Epoch 2, val loss: 0.09400103 -> 0.09265329, train loss: 0.09779492\n",
      "Epoch 3, val loss: 0.09265329 -> 0.09206702, train loss: 0.09685928\n",
      "Epoch 4, val loss: 0.09326412, train loss: 0.09666595\n",
      "Epoch 5, val loss: 0.09268960, train loss: 0.09620598\n",
      "Epoch 6, val loss: 0.09619924, train loss: 0.09628954\n",
      "Epoch 7, val loss: 0.09246180, train loss: 0.09590336\n",
      "Epoch 8, val loss: 0.09350681, train loss: 0.09523018\n",
      "Epoch 9, val loss: 0.09487033, train loss: 0.09524237\n",
      "f1: 0.6626396362350916, acc: 0.6632268086995118\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.12894786, train loss: 0.13546946\n",
      "Epoch 1, val loss: 0.12894786 -> 0.12356359, train loss: 0.12925322\n",
      "Epoch 2, val loss: 0.12661029, train loss: 0.12774185\n",
      "Epoch 3, val loss: 0.12356359 -> 0.12338408, train loss: 0.12735413\n",
      "Epoch 4, val loss: 0.12338408 -> 0.12301971, train loss: 0.12637471\n",
      "Epoch 5, val loss: 0.12301971 -> 0.12194271, train loss: 0.12579225\n",
      "Epoch 6, val loss: 0.12387051, train loss: 0.12508724\n",
      "Epoch 7, val loss: 0.13325441, train loss: 0.12478038\n",
      "Epoch 8, val loss: 0.12251587, train loss: 0.12477177\n",
      "Epoch 9, val loss: 0.12399859, train loss: 0.12383733\n",
      "f1: 0.5452798825766039, acc: 0.5415002219263204\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.15015919, train loss: 0.16211686\n",
      "Epoch 1, val loss: 0.15084992, train loss: 0.15450371\n",
      "Epoch 2, val loss: 0.15015919 -> 0.14975231, train loss: 0.15337625\n",
      "Epoch 3, val loss: 0.15241597, train loss: 0.15242199\n",
      "Epoch 4, val loss: 0.15026138, train loss: 0.15206221\n",
      "Epoch 5, val loss: 0.14975231 -> 0.14771006, train loss: 0.15129568\n",
      "Epoch 6, val loss: 0.15571199, train loss: 0.15102322\n",
      "Epoch 7, val loss: 0.14888657, train loss: 0.15074346\n",
      "Epoch 8, val loss: 0.14771006 -> 0.14731010, train loss: 0.14981694\n",
      "Epoch 9, val loss: 0.15364945, train loss: 0.14965200\n",
      "f1: 0.4337058077242796, acc: 0.46881935197514424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2013, 2017], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "    # Freeze encoder parameters to avoid CUDA out of memory.\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = UsefulScoreRegressorAllFeat(encoder, num_meta_feats=len(nonTextCols), outputs=num_vals)  # classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_text_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                          train_loader=train_loader, val_loader=val_loader,\n",
    "                          criterion=criterion,\n",
    "                          save_path=f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}_2013-2017_wAge.pt',\n",
    "                          clip=1.0,\n",
    "                          classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}_2013-2017_wAge.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT-META')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5195f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
