{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff5af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoModel\n",
    "import sys\n",
    "import math\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as perf\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import shuffle\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from utils.preprocessing import load_data\n",
    "from utils.transformer_dataset import ReviewDataset\n",
    "from utils.training import train_text_model, train_text_meta_model, train_meta_model\n",
    "from utils.evaluation import get_cls_perf, get_reg_perf, get_predictions\n",
    "from models.transformer_models import UsefulScoreRegressorTextOnly, UsefulScoreRegressorAllFeat, UsefulScoreRegressorMetaOnly\n",
    "from models.transformer_models import UsefulScoreRegressorLinearBaseline, DrugLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f9f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonTextCols = ['ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
    "               'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss', 'ratingNormalized']\n",
    "targetCol = 'usefulCountClass'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d4b80",
   "metadata": {},
   "source": [
    "# No usefulCount cap and no year filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595c992",
   "metadata": {},
   "source": [
    "# usefulCount cap of 99th percentile and years 2009 to 2013 filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3f33d",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d793f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.07397957, train loss: 0.07862058\n",
      "Epoch 1, val loss: 0.07397957 -> 0.07244341, train loss: 0.07569571\n",
      "Epoch 2, val loss: 0.07464819, train loss: 0.07410152\n",
      "Epoch 3, val loss: 0.07353341, train loss: 0.07354082\n",
      "Epoch 4, val loss: 0.07244341 -> 0.07212694, train loss: 0.07356331\n",
      "Epoch 5, val loss: 0.07239426, train loss: 0.07328345\n",
      "Epoch 6, val loss: 0.07257499, train loss: 0.07250335\n",
      "Epoch 7, val loss: 0.07244378, train loss: 0.07279101\n",
      "Epoch 8, val loss: 0.07227632, train loss: 0.07209536\n",
      "Epoch 9, val loss: 0.07458339, train loss: 0.07204516\n",
      "f1: 0.7084690178343485, acc: 0.7093584177520502\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.12647324, train loss: 0.12629476\n",
      "Epoch 1, val loss: 0.12647324 -> 0.12386728, train loss: 0.12218056\n",
      "Epoch 2, val loss: 0.12386728 -> 0.11931118, train loss: 0.12074414\n",
      "Epoch 3, val loss: 0.11931118 -> 0.11825148, train loss: 0.12030316\n",
      "Epoch 4, val loss: 0.12066339, train loss: 0.11919602\n",
      "Epoch 5, val loss: 0.12062639, train loss: 0.11900482\n",
      "Epoch 6, val loss: 0.11825148 -> 0.11792949, train loss: 0.11941074\n",
      "Epoch 7, val loss: 0.12064110, train loss: 0.11909745\n",
      "Epoch 8, val loss: 0.11959275, train loss: 0.11860086\n",
      "Epoch 9, val loss: 0.12241057, train loss: 0.11811673\n",
      "f1: 0.55352996697932, acc: 0.5487216594307767\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.15562239, train loss: 0.16038392\n",
      "Epoch 1, val loss: 0.15562239 -> 0.15348731, train loss: 0.15521719\n",
      "Epoch 2, val loss: 0.15846869, train loss: 0.15481228\n",
      "Epoch 3, val loss: 0.15348731 -> 0.15336014, train loss: 0.15331127\n",
      "Epoch 4, val loss: 0.15418938, train loss: 0.15333028\n",
      "Epoch 5, val loss: 0.15427197, train loss: 0.15270639\n",
      "Epoch 6, val loss: 0.15520688, train loss: 0.15295736\n",
      "Epoch 7, val loss: 0.15336014 -> 0.15173476, train loss: 0.15204734\n",
      "Epoch 8, val loss: 0.15173476 -> 0.15163752, train loss: 0.15141019\n",
      "Epoch 9, val loss: 0.15296284, train loss: 0.15108612\n",
      "f1: 0.39644252546605613, acc: 0.4339122045344911\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.18329719, train loss: 0.18750482\n",
      "Epoch 1, val loss: 0.18979755, train loss: 0.18342581\n",
      "Epoch 2, val loss: 0.18329719 -> 0.17997063, train loss: 0.18162689\n",
      "Epoch 3, val loss: 0.18400246, train loss: 0.18130777\n",
      "Epoch 4, val loss: 0.18065880, train loss: 0.18053920\n",
      "Epoch 5, val loss: 0.18199984, train loss: 0.18020246\n",
      "Epoch 6, val loss: 0.18007204, train loss: 0.18018453\n",
      "Epoch 7, val loss: 0.18137919, train loss: 0.18021469\n",
      "Epoch 8, val loss: 0.18195506, train loss: 0.17881925\n",
      "Epoch 9, val loss: 0.18371830, train loss: 0.17943231\n",
      "f1: 0.31877016609980846, acc: 0.3791606367583213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2009, 2013], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    ##### Text-only Transformer Model\n",
    "    encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "    # Freeze encoder parameters to avoid CUDA out of memory.\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = UsefulScoreRegressorTextOnly(encoder, outputs=num_vals)  # classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_text_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                     train_loader=train_loader, val_loader=val_loader,\n",
    "                     criterion=criterion,\n",
    "                     save_path=f'../models/Classifiers/distilBERT_Frozen_TextOnly_Classify{num_vals}.pt',\n",
    "                     clip=1.0,\n",
    "                     classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/distilBERT_Frozen_TextOnly_Classify{num_vals}.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da22258",
   "metadata": {},
   "source": [
    "#### Text-Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a042c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.07894914, train loss: 0.07412356\n",
      "Epoch 1, val loss: 0.07894914 -> 0.07483272, train loss: 0.07099397\n",
      "Epoch 2, val loss: 0.07483272 -> 0.07315473, train loss: 0.07054865\n",
      "Epoch 3, val loss: 0.07315473 -> 0.07162509, train loss: 0.06930481\n",
      "Epoch 4, val loss: 0.07162509 -> 0.07013874, train loss: 0.06984539\n",
      "Epoch 5, val loss: 0.08012029, train loss: 0.06891072\n",
      "Epoch 6, val loss: 0.07013874 -> 0.06934771, train loss: 0.06856231\n",
      "Epoch 7, val loss: 0.07182907, train loss: 0.06897483\n",
      "Epoch 8, val loss: 0.07228963, train loss: 0.06800907\n",
      "Epoch 9, val loss: 0.07055551, train loss: 0.06779471\n",
      "f1: 0.7241471284132024, acc: 0.7257597684515196\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.11995788, train loss: 0.12187148\n",
      "Epoch 1, val loss: 0.11995788 -> 0.11339937, train loss: 0.11671966\n",
      "Epoch 2, val loss: 0.11775153, train loss: 0.11595232\n",
      "Epoch 3, val loss: 0.11472886, train loss: 0.11498868\n",
      "Epoch 4, val loss: 0.11993646, train loss: 0.11457653\n",
      "Epoch 5, val loss: 0.11339937 -> 0.11240951, train loss: 0.11391138\n",
      "Epoch 6, val loss: 0.11562744, train loss: 0.11394991\n",
      "Epoch 7, val loss: 0.11792131, train loss: 0.11387904\n",
      "Epoch 8, val loss: 0.11668270, train loss: 0.11405753\n",
      "Epoch 9, val loss: 0.11559086, train loss: 0.11260378\n",
      "f1: 0.5806216338110883, acc: 0.58803666184274\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.14965397, train loss: 0.15494812\n",
      "Epoch 1, val loss: 0.14965397 -> 0.14812401, train loss: 0.14949239\n",
      "Epoch 2, val loss: 0.14812401 -> 0.14752596, train loss: 0.14777927\n",
      "Epoch 3, val loss: 0.14752596 -> 0.14702433, train loss: 0.14723278\n",
      "Epoch 4, val loss: 0.14712343, train loss: 0.14650919\n",
      "Epoch 5, val loss: 0.14722407, train loss: 0.14611327\n",
      "Epoch 6, val loss: 0.14702433 -> 0.14569425, train loss: 0.14681260\n",
      "Epoch 7, val loss: 0.14824853, train loss: 0.14539513\n",
      "Epoch 8, val loss: 0.14974093, train loss: 0.14493197\n",
      "Epoch 9, val loss: 0.14856823, train loss: 0.14463536\n",
      "f1: 0.4558750500635986, acc: 0.47081524360829713\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.17917263, train loss: 0.18179161\n",
      "Epoch 1, val loss: 0.17917263 -> 0.17722115, train loss: 0.17639215\n",
      "Epoch 2, val loss: 0.17722115 -> 0.17414836, train loss: 0.17524154\n",
      "Epoch 3, val loss: 0.17549802, train loss: 0.17421447\n",
      "Epoch 4, val loss: 0.17414836 -> 0.17370326, train loss: 0.17412958\n",
      "Epoch 5, val loss: 0.17706589, train loss: 0.17301640\n",
      "Epoch 6, val loss: 0.17469562, train loss: 0.17277673\n",
      "Epoch 7, val loss: 0.17370326 -> 0.17328874, train loss: 0.17240503\n",
      "Epoch 8, val loss: 0.17896442, train loss: 0.17202140\n",
      "Epoch 9, val loss: 0.17886223, train loss: 0.17137479\n",
      "f1: 0.33441931811374664, acc: 0.3958031837916064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2009, 2013], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "    # Freeze encoder parameters to avoid CUDA out of memory.\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = UsefulScoreRegressorAllFeat(encoder, num_meta_feats=len(nonTextCols), outputs=num_vals)  # classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_text_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                          train_loader=train_loader, val_loader=val_loader,\n",
    "                          criterion=criterion,\n",
    "                          save_path=f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}.pt',\n",
    "                          clip=1.0,\n",
    "                          classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT-META')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00212e0",
   "metadata": {},
   "source": [
    "#### Neural Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ac8e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.07052004, train loss: 0.07207060\n",
      "Epoch 1, val loss: 0.07052004 -> 0.07033605, train loss: 0.06990190\n",
      "Epoch 2, val loss: 0.07069781, train loss: 0.06976134\n",
      "Epoch 3, val loss: 0.07033605 -> 0.07023999, train loss: 0.06962311\n",
      "Epoch 4, val loss: 0.07023999 -> 0.07021992, train loss: 0.06960955\n",
      "Epoch 5, val loss: 0.07022868, train loss: 0.06945290\n",
      "Epoch 6, val loss: 0.07021992 -> 0.07010770, train loss: 0.06937224\n",
      "Epoch 7, val loss: 0.07018366, train loss: 0.06951573\n",
      "Epoch 8, val loss: 0.07010770 -> 0.07000520, train loss: 0.06945575\n",
      "Epoch 9, val loss: 0.07015875, train loss: 0.06935509\n",
      "f1: 0.7180882641386074, acc: 0.7240713941148095\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.11621064, train loss: 0.12217752\n",
      "Epoch 1, val loss: 0.11621064 -> 0.11573562, train loss: 0.11665317\n",
      "Epoch 2, val loss: 0.11605093, train loss: 0.11618662\n",
      "Epoch 3, val loss: 0.11573562 -> 0.11529325, train loss: 0.11612228\n",
      "Epoch 4, val loss: 0.11529325 -> 0.11522854, train loss: 0.11582194\n",
      "Epoch 5, val loss: 0.11526923, train loss: 0.11585897\n",
      "Epoch 6, val loss: 0.11522854 -> 0.11519532, train loss: 0.11575816\n",
      "Epoch 7, val loss: 0.11525842, train loss: 0.11544410\n",
      "Epoch 8, val loss: 0.11524299, train loss: 0.11547237\n",
      "Epoch 9, val loss: 0.11519532 -> 0.11507961, train loss: 0.11552458\n",
      "f1: 0.5662862314513024, acc: 0.5701881331403763\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.15051000, train loss: 0.15692528\n",
      "Epoch 1, val loss: 0.15051000 -> 0.14979828, train loss: 0.15012806\n",
      "Epoch 2, val loss: 0.14979828 -> 0.14950043, train loss: 0.14928409\n",
      "Epoch 3, val loss: 0.14950043 -> 0.14930727, train loss: 0.14900997\n",
      "Epoch 4, val loss: 0.14937600, train loss: 0.14870765\n",
      "Epoch 5, val loss: 0.14930727 -> 0.14909203, train loss: 0.14869126\n",
      "Epoch 6, val loss: 0.14909203 -> 0.14896609, train loss: 0.14859520\n",
      "Epoch 7, val loss: 0.14908025, train loss: 0.14801536\n",
      "Epoch 8, val loss: 0.14906016, train loss: 0.14802056\n",
      "Epoch 9, val loss: 0.14896609 -> 0.14893163, train loss: 0.14835315\n",
      "f1: 0.4401732209022009, acc: 0.46526772793053545\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.17843647, train loss: 0.18647259\n",
      "Epoch 1, val loss: 0.17843647 -> 0.17716672, train loss: 0.17732596\n",
      "Epoch 2, val loss: 0.17716672 -> 0.17685632, train loss: 0.17663052\n",
      "Epoch 3, val loss: 0.17685632 -> 0.17656053, train loss: 0.17640285\n",
      "Epoch 4, val loss: 0.17656053 -> 0.17633520, train loss: 0.17635338\n",
      "Epoch 5, val loss: 0.17633520 -> 0.17624543, train loss: 0.17582585\n",
      "Epoch 6, val loss: 0.17624543 -> 0.17612801, train loss: 0.17597749\n",
      "Epoch 7, val loss: 0.17643606, train loss: 0.17569619\n",
      "Epoch 8, val loss: 0.17614919, train loss: 0.17546150\n",
      "Epoch 9, val loss: 0.17612801 -> 0.17606180, train loss: 0.17552457\n",
      "f1: 0.32964193206641135, acc: 0.39725036179450074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2009, 2013], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'roberta-base', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'roberta-base', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = UsefulScoreRegressorMetaOnly(num_meta_feats=len(nonTextCols), outputs=num_vals)  #classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                     train_loader=train_loader, val_loader=val_loader,\n",
    "                     criterion=criterion, save_path=f'../models/Classifiers/MetaOnly_NNClassifierBaseline{num_vals}.pt', clip=10000.0,\n",
    "                     classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/MetaOnly_NNClassifierBaseline{num_vals}.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='META')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257da775",
   "metadata": {},
   "source": [
    "#### Linear Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97198c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.07131396, train loss: 0.07458621\n",
      "Epoch 1, val loss: 0.07131396 -> 0.07051269, train loss: 0.06978545\n",
      "Epoch 2, val loss: 0.07051269 -> 0.07040187, train loss: 0.06936264\n",
      "Epoch 3, val loss: 0.07040187 -> 0.07034733, train loss: 0.06928108\n",
      "Epoch 4, val loss: 0.07034733 -> 0.07028681, train loss: 0.06922653\n",
      "Epoch 5, val loss: 0.07028681 -> 0.07028193, train loss: 0.06918832\n",
      "Epoch 6, val loss: 0.07028791, train loss: 0.06915965\n",
      "Epoch 7, val loss: 0.07031276, train loss: 0.06917242\n",
      "Epoch 8, val loss: 0.07028193 -> 0.07026925, train loss: 0.06916486\n",
      "Epoch 9, val loss: 0.07029370, train loss: 0.06915908\n",
      "f1: 0.7189442112730411, acc: 0.7243125904486252\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.11900895, train loss: 0.12557110\n",
      "Epoch 1, val loss: 0.11900895 -> 0.11664075, train loss: 0.11730793\n",
      "Epoch 2, val loss: 0.11664075 -> 0.11610938, train loss: 0.11605261\n",
      "Epoch 3, val loss: 0.11610938 -> 0.11590185, train loss: 0.11564947\n",
      "Epoch 4, val loss: 0.11590185 -> 0.11572416, train loss: 0.11548662\n",
      "Epoch 5, val loss: 0.11572416 -> 0.11565953, train loss: 0.11535283\n",
      "Epoch 6, val loss: 0.11565953 -> 0.11560272, train loss: 0.11525156\n",
      "Epoch 7, val loss: 0.11560272 -> 0.11548736, train loss: 0.11521767\n",
      "Epoch 8, val loss: 0.11548736 -> 0.11545989, train loss: 0.11514328\n",
      "Epoch 9, val loss: 0.11545989 -> 0.11538316, train loss: 0.11512379\n",
      "f1: 0.5602546392587925, acc: 0.5675349734684033\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.15436999, train loss: 0.16176224\n",
      "Epoch 1, val loss: 0.15436999 -> 0.15077666, train loss: 0.15094656\n",
      "Epoch 2, val loss: 0.15077666 -> 0.14990958, train loss: 0.14893298\n",
      "Epoch 3, val loss: 0.14990958 -> 0.14962220, train loss: 0.14834220\n",
      "Epoch 4, val loss: 0.14962220 -> 0.14951047, train loss: 0.14811896\n",
      "Epoch 5, val loss: 0.14951047 -> 0.14939434, train loss: 0.14796760\n",
      "Epoch 6, val loss: 0.14939434 -> 0.14938857, train loss: 0.14785599\n",
      "Epoch 7, val loss: 0.14938857 -> 0.14918286, train loss: 0.14779984\n",
      "Epoch 8, val loss: 0.14918286 -> 0.14916898, train loss: 0.14772097\n",
      "Epoch 9, val loss: 0.14916898 -> 0.14906734, train loss: 0.14765093\n",
      "f1: 0.4335050980677665, acc: 0.4592378195851423\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.18256414, train loss: 0.18872958\n",
      "Epoch 1, val loss: 0.18256414 -> 0.17874049, train loss: 0.17898802\n",
      "Epoch 2, val loss: 0.17874049 -> 0.17772423, train loss: 0.17671825\n",
      "Epoch 3, val loss: 0.17772423 -> 0.17733473, train loss: 0.17600048\n",
      "Epoch 4, val loss: 0.17733473 -> 0.17718949, train loss: 0.17559949\n",
      "Epoch 5, val loss: 0.17718949 -> 0.17698422, train loss: 0.17541350\n",
      "Epoch 6, val loss: 0.17698422 -> 0.17692909, train loss: 0.17524176\n",
      "Epoch 7, val loss: 0.17692909 -> 0.17674701, train loss: 0.17511775\n",
      "Epoch 8, val loss: 0.17674701 -> 0.17661244, train loss: 0.17503556\n",
      "Epoch 9, val loss: 0.17661244 -> 0.17654041, train loss: 0.17495843\n",
      "f1: 0.3239079237830075, acc: 0.3888084901109503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2009, 2013], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'roberta-base', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'roberta-base', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = UsefulScoreRegressorLinearBaseline(num_meta_feats=len(nonTextCols), outputs=num_vals)  #classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                     train_loader=train_loader, val_loader=val_loader,\n",
    "                     criterion=criterion, save_path=f'../models/Classifiers/MetaOnly_LinearClassifierBaseline{num_vals}.pt', clip=10000.0,\n",
    "                     classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/MetaOnly_LinearClassifierBaseline{num_vals}.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='META')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b6d1e",
   "metadata": {},
   "source": [
    "# usefulCount cap of 99th percentile and years 2013 to 2017 with Age Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105dc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonTextCols = ['ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
    "               'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss', 'ratingNormalized', 'ageScore']\n",
    "targetCol = 'usefulCountClass'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02bdb2",
   "metadata": {},
   "source": [
    "#### Text-Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef891a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.05181526, train loss: 0.06054873\n",
      "Epoch 1, val loss: 0.05279917, train loss: 0.05560468\n",
      "Epoch 2, val loss: 0.05197458, train loss: 0.05457137\n",
      "Epoch 3, val loss: 0.05181526 -> 0.05133883, train loss: 0.05420045\n",
      "Epoch 4, val loss: 0.05414853, train loss: 0.05338410\n",
      "Epoch 5, val loss: 0.05154473, train loss: 0.05308257\n",
      "Epoch 6, val loss: 0.05201705, train loss: 0.05276312\n",
      "Epoch 7, val loss: 0.05191622, train loss: 0.05284573\n",
      "Epoch 8, val loss: 0.05620904, train loss: 0.05252063\n",
      "Epoch 9, val loss: 0.05133883 -> 0.05133720, train loss: 0.05197617\n",
      "f1: 0.8136900438206434, acc: 0.8136928539724811\n",
      "\n",
      "num_vals: 3\n",
      "curr_quantiles: [0.33333333 0.66666667]\n",
      "Epoch 0, val loss: inf -> 0.09994811, train loss: 0.10484060\n",
      "Epoch 1, val loss: 0.09994811 -> 0.09400103, train loss: 0.09878587\n",
      "Epoch 2, val loss: 0.09400103 -> 0.09265329, train loss: 0.09779492\n",
      "Epoch 3, val loss: 0.09265329 -> 0.09206702, train loss: 0.09685928\n",
      "Epoch 4, val loss: 0.09326412, train loss: 0.09666595\n",
      "Epoch 5, val loss: 0.09268960, train loss: 0.09620598\n",
      "Epoch 6, val loss: 0.09619924, train loss: 0.09628954\n",
      "Epoch 7, val loss: 0.09246180, train loss: 0.09590336\n",
      "Epoch 8, val loss: 0.09350681, train loss: 0.09523018\n",
      "Epoch 9, val loss: 0.09487033, train loss: 0.09524237\n",
      "f1: 0.6626396362350916, acc: 0.6632268086995118\n",
      "\n",
      "num_vals: 4\n",
      "curr_quantiles: [0.25 0.5  0.75]\n",
      "Epoch 0, val loss: inf -> 0.12894786, train loss: 0.13546946\n",
      "Epoch 1, val loss: 0.12894786 -> 0.12356359, train loss: 0.12925322\n",
      "Epoch 2, val loss: 0.12661029, train loss: 0.12774185\n",
      "Epoch 3, val loss: 0.12356359 -> 0.12338408, train loss: 0.12735413\n",
      "Epoch 4, val loss: 0.12338408 -> 0.12301971, train loss: 0.12637471\n",
      "Epoch 5, val loss: 0.12301971 -> 0.12194271, train loss: 0.12579225\n",
      "Epoch 6, val loss: 0.12387051, train loss: 0.12508724\n",
      "Epoch 7, val loss: 0.13325441, train loss: 0.12478038\n",
      "Epoch 8, val loss: 0.12251587, train loss: 0.12477177\n",
      "Epoch 9, val loss: 0.12399859, train loss: 0.12383733\n",
      "f1: 0.5452798825766039, acc: 0.5415002219263204\n",
      "\n",
      "num_vals: 5\n",
      "curr_quantiles: [0.2 0.4 0.6 0.8]\n",
      "Epoch 0, val loss: inf -> 0.15015919, train loss: 0.16211686\n",
      "Epoch 1, val loss: 0.15084992, train loss: 0.15450371\n",
      "Epoch 2, val loss: 0.15015919 -> 0.14975231, train loss: 0.15337625\n",
      "Epoch 3, val loss: 0.15241597, train loss: 0.15242199\n",
      "Epoch 4, val loss: 0.15026138, train loss: 0.15206221\n",
      "Epoch 5, val loss: 0.14975231 -> 0.14771006, train loss: 0.15129568\n",
      "Epoch 6, val loss: 0.15571199, train loss: 0.15102322\n",
      "Epoch 7, val loss: 0.14888657, train loss: 0.15074346\n",
      "Epoch 8, val loss: 0.14771006 -> 0.14731010, train loss: 0.14981694\n",
      "Epoch 9, val loss: 0.15364945, train loss: 0.14965200\n",
      "f1: 0.4337058077242796, acc: 0.46881935197514424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_vals in range(2, 6):\n",
    "    curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "    print(f'num_vals: {num_vals}')\n",
    "    print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "    train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2013, 2017], usefulCount_range=[0, 10000],\n",
    "                           quantiles_for_class=curr_quantiles)\n",
    "\n",
    "    trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "    train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "    encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "    # Freeze encoder parameters to avoid CUDA out of memory.\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model = UsefulScoreRegressorAllFeat(encoder, num_meta_feats=len(nonTextCols), outputs=num_vals)  # classification\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train_text_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                          train_loader=train_loader, val_loader=val_loader,\n",
    "                          criterion=criterion,\n",
    "                          save_path=f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}_2013-2017_wAge.pt',\n",
    "                          clip=1.0,\n",
    "                          classify=True)\n",
    "\n",
    "    #### Load the best model for the training run and evaluate its performance\n",
    "    model = torch.load(f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}_2013-2017_wAge.pt')\n",
    "    f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT-META')\n",
    "    print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1add72",
   "metadata": {},
   "source": [
    "#### TextMeta with No Age (2013 to 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e5195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.05997521, train loss: 0.06635515\n",
      "Epoch 1, val loss: 0.05997521 -> 0.05889097, train loss: 0.06260199\n",
      "Epoch 2, val loss: 0.05889097 -> 0.05849366, train loss: 0.06222812\n",
      "Epoch 3, val loss: 0.05899091, train loss: 0.06151160\n",
      "Epoch 4, val loss: 0.06324222, train loss: 0.06089569\n",
      "Epoch 5, val loss: 0.06426434, train loss: 0.06142596\n",
      "Epoch 6, val loss: 0.06211168, train loss: 0.06135753\n",
      "Epoch 7, val loss: 0.06107429, train loss: 0.06115998\n",
      "Epoch 8, val loss: 0.06239430, train loss: 0.06115048\n",
      "Epoch 9, val loss: 0.05891140, train loss: 0.06072957\n",
      "f1: 0.7941301965824246, acc: 0.7944962272525522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonTextCols = ['ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
    "               'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss', 'ratingNormalized']\n",
    "targetCol = 'usefulCountClass'\n",
    "\n",
    "num_vals = 2\n",
    "curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "print(f'num_vals: {num_vals}')\n",
    "print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2013, 2017], usefulCount_range=[0, 10000],\n",
    "                       quantiles_for_class=curr_quantiles)\n",
    "\n",
    "trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "# Freeze encoder parameters to avoid CUDA out of memory.\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = UsefulScoreRegressorAllFeat(encoder, num_meta_feats=len(nonTextCols), outputs=num_vals)  # classification\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_text_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                      train_loader=train_loader, val_loader=val_loader,\n",
    "                      criterion=criterion,\n",
    "                      save_path=f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}_2013-2017_NoAge.pt',\n",
    "                      clip=1.0,\n",
    "                      classify=True)\n",
    "\n",
    "#### Load the best model for the training run and evaluate its performance\n",
    "model = torch.load(f'../models/Classifiers/distilBERT_Frozen_TextMeta_Classify{num_vals}_2013-2017_NoAge.pt')\n",
    "f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT-META')\n",
    "print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e5bae",
   "metadata": {},
   "source": [
    "#### Text Only with No Age (2013 to 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb53ba4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.06722652, train loss: 0.07265359\n",
      "Epoch 1, val loss: 0.06722652 -> 0.06472123, train loss: 0.06957825\n",
      "Epoch 2, val loss: 0.06523891, train loss: 0.06883857\n",
      "Epoch 3, val loss: 0.06472123 -> 0.06415356, train loss: 0.06808269\n",
      "Epoch 4, val loss: 0.06561413, train loss: 0.06774625\n",
      "Epoch 5, val loss: 0.06636589, train loss: 0.06766147\n",
      "Epoch 6, val loss: 0.06530433, train loss: 0.06785676\n",
      "Epoch 7, val loss: 0.06534453, train loss: 0.06777894\n",
      "Epoch 8, val loss: 0.06539893, train loss: 0.06768081\n",
      "Epoch 9, val loss: 0.06858623, train loss: 0.06728828\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7607e23f3582>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#### Load the best model for the training run and evaluate its performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'../models/Classifiers/distilBERT_Frozen_TextOnly_Classify{num_vals}_2013-2017_NoAge.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cls_perf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TEXT-META'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'f1: {f1}, acc: {acc}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GT\\MSCS\\Fall_2021\\CS7650_NLP\\Final_Project\\DrugReviews\\GitHub\\NLP_Final_Project\\utils\\evaluation.py\u001b[0m in \u001b[0;36mget_cls_perf\u001b[1;34m(model, loader, model_type)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_cls_perf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CLASSIFICATION'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GT\\MSCS\\Fall_2021\\CS7650_NLP\\Final_Project\\DrugReviews\\GitHub\\NLP_Final_Project\\utils\\evaluation.py\u001b[0m in \u001b[0;36mget_predictions\u001b[1;34m(model, loader, model_type, task, curr_buckets, max_usefulCount)\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'TEXT-META'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid model_type: should be meta, text, or text-meta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "targetCol = 'usefulCountClass'\n",
    "\n",
    "num_vals = 2\n",
    "curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "print(f'num_vals: {num_vals}')\n",
    "print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2013, 2017], usefulCount_range=[0, 10000],\n",
    "                       quantiles_for_class=curr_quantiles)\n",
    "\n",
    "trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "# Freeze encoder parameters to avoid CUDA out of memory.\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = UsefulScoreRegressorTextOnly(encoder, outputs=num_vals)  # classification\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_text_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                 train_loader=train_loader, val_loader=val_loader,\n",
    "                 criterion=criterion,\n",
    "                 save_path=f'../models/Classifiers/distilBERT_Frozen_TextOnly_Classify{num_vals}_2013-2017_NoAge.pt',\n",
    "                 clip=1.0,\n",
    "                 classify=True)\n",
    "\n",
    "#### Load the best model for the training run and evaluate its performance\n",
    "model = torch.load(f'../models/Classifiers/distilBERT_Frozen_TextOnly_Classify{num_vals}_2013-2017_NoAge.pt')\n",
    "f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT-META')\n",
    "print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbb599",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd1cdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.05537779, train loss: 0.06314491\n",
      "Epoch 1, val loss: 0.05537779 -> 0.05337884, train loss: 0.05370179\n",
      "Epoch 2, val loss: 0.05337884 -> 0.05283048, train loss: 0.05238302\n",
      "Epoch 3, val loss: 0.05283048 -> 0.05272780, train loss: 0.05200583\n",
      "Epoch 4, val loss: 0.05274979, train loss: 0.05187185\n",
      "Epoch 5, val loss: 0.05272780 -> 0.05270569, train loss: 0.05181751\n",
      "Epoch 6, val loss: 0.05270569 -> 0.05268513, train loss: 0.05179731\n",
      "Epoch 7, val loss: 0.05278042, train loss: 0.05180243\n",
      "Epoch 8, val loss: 0.05270122, train loss: 0.05181377\n",
      "Epoch 9, val loss: 0.05272230, train loss: 0.05178892\n",
      "f1: 0.8088902083824444, acc: 0.8091433644030182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonTextCols = ['ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
    "               'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss', 'ratingNormalized', 'ageScore']\n",
    "targetCol = 'usefulCountClass'\n",
    "\n",
    "num_vals = 2\n",
    "curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "print(f'num_vals: {num_vals}')\n",
    "print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2013, 2017], usefulCount_range=[0, 10000],\n",
    "                       quantiles_for_class=curr_quantiles)\n",
    "\n",
    "trainset = ReviewDataset(train, 'roberta-base', nonTextCols, targetCol)\n",
    "valset = ReviewDataset(val, 'roberta-base', nonTextCols, targetCol)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "model = UsefulScoreRegressorLinearBaseline(num_meta_feats=len(nonTextCols), outputs=num_vals)  #classification\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                 train_loader=train_loader, val_loader=val_loader,\n",
    "                 criterion=criterion, save_path=f'../models/Classifiers/MetaOnly_LinearClassifierBaseline{num_vals}_2013-2017_wAge.pt', clip=10000.0,\n",
    "                 classify=True)\n",
    "\n",
    "#### Load the best model for the training run and evaluate its performance\n",
    "model = torch.load(f'../models/Classifiers/MetaOnly_LinearClassifierBaseline{num_vals}_2013-2017_wAge.pt')\n",
    "f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='META')\n",
    "print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd5002",
   "metadata": {},
   "source": [
    "#### Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5cdc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.05249501, train loss: 0.05553002\n",
      "Epoch 1, val loss: 0.05249501 -> 0.05199696, train loss: 0.05234200\n",
      "Epoch 2, val loss: 0.05199696 -> 0.05188889, train loss: 0.05183899\n",
      "Epoch 3, val loss: 0.05188889 -> 0.05179347, train loss: 0.05184594\n",
      "Epoch 4, val loss: 0.05179347 -> 0.05176263, train loss: 0.05165942\n",
      "Epoch 5, val loss: 0.05176263 -> 0.05161766, train loss: 0.05152232\n",
      "Epoch 6, val loss: 0.05161766 -> 0.05159758, train loss: 0.05152785\n",
      "Epoch 7, val loss: 0.05159758 -> 0.05149618, train loss: 0.05128081\n",
      "Epoch 8, val loss: 0.05149618 -> 0.05147432, train loss: 0.05132696\n",
      "Epoch 9, val loss: 0.05147432 -> 0.05139847, train loss: 0.05121005\n",
      "f1: 0.8116571900798037, acc: 0.8119174434087882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonTextCols = ['ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
    "               'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss', 'ratingNormalized', 'ageScore']\n",
    "targetCol = 'usefulCountClass'\n",
    "\n",
    "num_vals = 2\n",
    "curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "print(f'num_vals: {num_vals}')\n",
    "print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2013, 2017], usefulCount_range=[0, 10000],\n",
    "                       quantiles_for_class=curr_quantiles)\n",
    "\n",
    "trainset = ReviewDataset(train, 'roberta-base', nonTextCols, targetCol)\n",
    "valset = ReviewDataset(val, 'roberta-base', nonTextCols, targetCol)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)\n",
    "\n",
    "model = UsefulScoreRegressorMetaOnly(num_meta_feats=len(nonTextCols), outputs=num_vals)  #classification\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                 train_loader=train_loader, val_loader=val_loader,\n",
    "                 criterion=criterion, save_path=f'../models/Classifiers/MetaOnly_NeuralClassifierBaseline{num_vals}_2013-2017_wAge.pt', clip=10000.0,\n",
    "                 classify=True)\n",
    "\n",
    "#### Load the best model for the training run and evaluate its performance\n",
    "model = torch.load(f'../models/Classifiers/MetaOnly_NeuralClassifierBaseline{num_vals}_2013-2017_wAge.pt')\n",
    "f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='META')\n",
    "print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cf67a",
   "metadata": {},
   "source": [
    "#### Text Meta Free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "717be08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_vals: 2\n",
      "curr_quantiles: [0.5]\n",
      "Epoch 0, val loss: inf -> 0.26000984, train loss: 0.26243770\n",
      "Epoch 1, val loss: 0.26708783, train loss: 0.24437089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5523c87af0c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m                       \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'../models/Classifiers/distilBERT_Free_TextMeta_Classify{num_vals}_2013-2017_wAge.pt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                       \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                       classify=True)\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#### Load the best model for the training run and evaluate its performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GT\\MSCS\\Fall_2021\\CS7650_NLP\\Final_Project\\DrugReviews\\GitHub\\NLP_Final_Project\\utils\\training.py\u001b[0m in \u001b[0;36mtrain_text_meta_model\u001b[1;34m(num_epochs, model, optimizer, train_loader, val_loader, criterion, save_path, clip, classify)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mtot_train_samples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# get gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# clip gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nonTextCols = ['ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde', 'Birth Control',\n",
    "               'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss', 'ratingNormalized', 'ageScore']\n",
    "targetCol = 'usefulCountClass'\n",
    "\n",
    "num_vals = 2\n",
    "\n",
    "curr_quantiles = np.array([(1/num_vals)*i for i in range(1, num_vals)])\n",
    "print(f'num_vals: {num_vals}')\n",
    "print(f'curr_quantiles: {curr_quantiles}')\n",
    "\n",
    "train, val = load_data('../data/drugsComTrain_raw.csv', year_range=[2013, 2017], usefulCount_range=[0, 10000],\n",
    "                       quantiles_for_class=curr_quantiles)\n",
    "\n",
    "trainset = ReviewDataset(train, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "valset = ReviewDataset(val, 'distilbert-base-uncased', nonTextCols, targetCol)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valset, batch_size=2, shuffle=False)\n",
    "\n",
    "encoder = AutoModel.from_pretrained('distilbert-base-uncased', return_dict=True)\n",
    "\n",
    "# Freeze encoder parameters to avoid CUDA out of memory.\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = UsefulScoreRegressorAllFeat(encoder, num_meta_feats=len(nonTextCols), outputs=num_vals)  # classification\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_text_meta_model(num_epochs=10, model=model, optimizer=optimizer,\n",
    "                      train_loader=train_loader, val_loader=val_loader,\n",
    "                      criterion=criterion,\n",
    "                      save_path=f'../models/Classifiers/distilBERT_Free_TextMeta_Classify{num_vals}_2013-2017_wAge.pt',\n",
    "                      clip=1.0,\n",
    "                      classify=True)\n",
    "\n",
    "#### Load the best model for the training run and evaluate its performance\n",
    "model = torch.load(f'../models/Classifiers/distilBERT_Free_TextMeta_Classify{num_vals}_2013-2017_wAge.pt')\n",
    "f1, acc = get_cls_perf(model=model, loader=val_loader, model_type='TEXT-META')\n",
    "print(f'f1: {f1}, acc: {acc}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73682c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
