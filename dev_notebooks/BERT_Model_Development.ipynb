{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91e6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoModel\n",
    "import sys\n",
    "import math\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from utils.preprocessing import load_data\n",
    "from utils.transformer_dataset import ReviewDataset\n",
    "from utils.training import train_text_model, train_text_meta_model, train_meta_model\n",
    "from models.transformer_models import UsefulScoreRegressorTextOnly, UsefulScoreRegressorAllFeat, UsefulScoreRegressorMetaOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96f8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train, val = load_data('../data/drugsComTrain_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0519b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>cleanReview</th>\n",
       "      <th>usefulScore</th>\n",
       "      <th>ratingNormalized</th>\n",
       "      <th>...</th>\n",
       "      <th>ADHD</th>\n",
       "      <th>Acne</th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Bipolar Disorde</th>\n",
       "      <th>Birth Control</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Insomnia</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Pain</th>\n",
       "      <th>Weight Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126080</th>\n",
       "      <td>110122</td>\n",
       "      <td>Nexplanon</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"Hello, Ive had Nexplanon for four years (just...</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>9</td>\n",
       "      <td>Hello, Ive had Nexplanon for four years (just ...</td>\n",
       "      <td>0.306739</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123803</th>\n",
       "      <td>6499</td>\n",
       "      <td>Wellbutrin XL</td>\n",
       "      <td>Depression</td>\n",
       "      <td>\"I started taking Wellbutrin XL August of 2016...</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-09-15</td>\n",
       "      <td>16</td>\n",
       "      <td>I started taking Wellbutrin XL August of 2016....</td>\n",
       "      <td>0.387062</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35971</th>\n",
       "      <td>39194</td>\n",
       "      <td>Contrave</td>\n",
       "      <td>Weight Loss</td>\n",
       "      <td>\"I tried Contrave for 5 days. I was just takin...</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-04-19</td>\n",
       "      <td>8</td>\n",
       "      <td>I tried Contrave for 5 days. I was just taking...</td>\n",
       "      <td>0.290296</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38384</th>\n",
       "      <td>137414</td>\n",
       "      <td>Isotretinoin</td>\n",
       "      <td>Acne</td>\n",
       "      <td>\"Accutane is an isotretinoin- the most powerfu...</td>\n",
       "      <td>10</td>\n",
       "      <td>2010-12-13</td>\n",
       "      <td>13</td>\n",
       "      <td>Accutane is an isotretinoin- the most powerful...</td>\n",
       "      <td>0.358074</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89258</th>\n",
       "      <td>211662</td>\n",
       "      <td>Lamotrigine</td>\n",
       "      <td>Bipolar Disorde</td>\n",
       "      <td>\"I started on Lamictal after having manic epis...</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>39</td>\n",
       "      <td>I started on Lamictal after having manic episo...</td>\n",
       "      <td>0.511444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25083</th>\n",
       "      <td>104070</td>\n",
       "      <td>Ethinyl estradiol / levonorgestrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"My favorite birth control by far! I have had ...</td>\n",
       "      <td>10</td>\n",
       "      <td>2013-11-13</td>\n",
       "      <td>22</td>\n",
       "      <td>My favorite birth control by far! I have had n...</td>\n",
       "      <td>0.431519</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129693</th>\n",
       "      <td>210966</td>\n",
       "      <td>Mirtazapine</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>\"At 19 I went on Sertraline as I&amp;#039;d strugg...</td>\n",
       "      <td>9</td>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>65</td>\n",
       "      <td>At 19 I went on Sertraline as I'd struggled wi...</td>\n",
       "      <td>0.582757</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62647</th>\n",
       "      <td>220149</td>\n",
       "      <td>Amitriptyline</td>\n",
       "      <td>Pain</td>\n",
       "      <td>\"I&amp;#039;m 20 years old and have both fibromyal...</td>\n",
       "      <td>10</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>100</td>\n",
       "      <td>I'm 20 years old and have both fibromyalgia an...</td>\n",
       "      <td>0.642895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147595</th>\n",
       "      <td>45327</td>\n",
       "      <td>Fluoxetine</td>\n",
       "      <td>Depression</td>\n",
       "      <td>\"I was prescribed prozac for depression about ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>13</td>\n",
       "      <td>I was prescribed prozac for depression about 3...</td>\n",
       "      <td>0.358074</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86257</th>\n",
       "      <td>84826</td>\n",
       "      <td>Ethinyl estradiol / norgestimate</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I began taking this medicine primarily to not...</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>7</td>\n",
       "      <td>I began taking this medicine primarily to not ...</td>\n",
       "      <td>0.271655</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55463 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uniqueID                            drugName        condition  \\\n",
       "126080    110122                           Nexplanon    Birth Control   \n",
       "123803      6499                       Wellbutrin XL       Depression   \n",
       "35971      39194                            Contrave      Weight Loss   \n",
       "38384     137414                        Isotretinoin             Acne   \n",
       "89258     211662                         Lamotrigine  Bipolar Disorde   \n",
       "...          ...                                 ...              ...   \n",
       "25083     104070  Ethinyl estradiol / levonorgestrel    Birth Control   \n",
       "129693    210966                         Mirtazapine          Anxiety   \n",
       "62647     220149                       Amitriptyline             Pain   \n",
       "147595     45327                          Fluoxetine       Depression   \n",
       "86257      84826    Ethinyl estradiol / norgestimate    Birth Control   \n",
       "\n",
       "                                                   review  rating       date  \\\n",
       "126080  \"Hello, Ive had Nexplanon for four years (just...       8 2016-04-19   \n",
       "123803  \"I started taking Wellbutrin XL August of 2016...      10 2017-09-15   \n",
       "35971   \"I tried Contrave for 5 days. I was just takin...       5 2017-04-19   \n",
       "38384   \"Accutane is an isotretinoin- the most powerfu...      10 2010-12-13   \n",
       "89258   \"I started on Lamictal after having manic epis...      10 2017-07-10   \n",
       "...                                                   ...     ...        ...   \n",
       "25083   \"My favorite birth control by far! I have had ...      10 2013-11-13   \n",
       "129693  \"At 19 I went on Sertraline as I&#039;d strugg...       9 2015-11-29   \n",
       "62647   \"I&#039;m 20 years old and have both fibromyal...      10 2015-08-17   \n",
       "147595  \"I was prescribed prozac for depression about ...       8 2015-08-10   \n",
       "86257   \"I began taking this medicine primarily to not...       6 2016-04-13   \n",
       "\n",
       "        usefulCount                                        cleanReview  \\\n",
       "126080            9  Hello, Ive had Nexplanon for four years (just ...   \n",
       "123803           16  I started taking Wellbutrin XL August of 2016....   \n",
       "35971             8  I tried Contrave for 5 days. I was just taking...   \n",
       "38384            13  Accutane is an isotretinoin- the most powerful...   \n",
       "89258            39  I started on Lamictal after having manic episo...   \n",
       "...             ...                                                ...   \n",
       "25083            22  My favorite birth control by far! I have had n...   \n",
       "129693           65  At 19 I went on Sertraline as I'd struggled wi...   \n",
       "62647           100  I'm 20 years old and have both fibromyalgia an...   \n",
       "147595           13  I was prescribed prozac for depression about 3...   \n",
       "86257             7  I began taking this medicine primarily to not ...   \n",
       "\n",
       "        usefulScore  ratingNormalized  ...  ADHD  Acne  Anxiety  \\\n",
       "126080     0.306739               0.8  ...     0     0        0   \n",
       "123803     0.387062               1.0  ...     0     0        0   \n",
       "35971      0.290296               0.5  ...     0     0        0   \n",
       "38384      0.358074               1.0  ...     0     1        0   \n",
       "89258      0.511444               1.0  ...     0     0        0   \n",
       "...             ...               ...  ...   ...   ...      ...   \n",
       "25083      0.431519               1.0  ...     0     0        0   \n",
       "129693     0.582757               0.9  ...     0     0        1   \n",
       "62647      0.642895               1.0  ...     0     0        0   \n",
       "147595     0.358074               0.8  ...     0     0        0   \n",
       "86257      0.271655               0.6  ...     0     0        0   \n",
       "\n",
       "        Bipolar Disorde  Birth Control  Depression  Insomnia  Obesity  Pain  \\\n",
       "126080                0              1           0         0        0     0   \n",
       "123803                0              0           1         0        0     0   \n",
       "35971                 0              0           0         0        0     0   \n",
       "38384                 0              0           0         0        0     0   \n",
       "89258                 1              0           0         0        0     0   \n",
       "...                 ...            ...         ...       ...      ...   ...   \n",
       "25083                 0              1           0         0        0     0   \n",
       "129693                0              0           0         0        0     0   \n",
       "62647                 0              0           0         0        0     1   \n",
       "147595                0              0           1         0        0     0   \n",
       "86257                 0              1           0         0        0     0   \n",
       "\n",
       "        Weight Loss  \n",
       "126080            0  \n",
       "123803            0  \n",
       "35971             1  \n",
       "38384             0  \n",
       "89258             0  \n",
       "...             ...  \n",
       "25083             0  \n",
       "129693            0  \n",
       "62647             0  \n",
       "147595            0  \n",
       "86257             0  \n",
       "\n",
       "[55463 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See dataframe\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83d7e6",
   "metadata": {},
   "source": [
    "#### Develop BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ca7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create pytorch dataset\n",
    "nonTextCols = ['ratingNormalized', 'ageScore', 'ADHD', 'Acne', 'Anxiety', 'Bipolar Disorde',\n",
    "                'Birth Control', 'Depression', 'Insomnia', 'Obesity', 'Pain', 'Weight Loss']\n",
    "targetCol = 'usefulScore'\n",
    "\n",
    "trainset = ReviewDataset(train, 'roberta-base', nonTextCols, targetCol)\n",
    "valset = ReviewDataset(val, 'roberta-base', nonTextCols, targetCol)\n",
    "train_loader = DataLoader(dataset=trainset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c402b93",
   "metadata": {},
   "source": [
    "#### Train Models (Frozen Transformer Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dcd53d",
   "metadata": {},
   "source": [
    "#### Text-only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169f762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, val loss: inf -> 0.00274, train loss: 0.00334\n",
      "Epoch 1, val loss: 0.00274 -> 0.00273, train loss: 0.00307\n",
      "Epoch 2, val loss: 0.00293, train loss: 0.00305\n"
     ]
    }
   ],
   "source": [
    "##### Text-only Transformer Model\n",
    "encoder = AutoModel.from_pretrained('roberta-base', return_dict=True)\n",
    "\n",
    "# Freeze encoder parameters to avoid CUDA out of memory.\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = UsefulScoreRegressorTextOnly(encoder)\n",
    "model = model.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_text_model(num_epochs=3, model=model, optimizer=optimizer,\n",
    "                 train_loader=train_loader, val_loader=val_loader,\n",
    "                 criterion=criterion, save_path='../models/RoBERTa_Frozen_TextOnly_Clip.pt', clip=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2056ea6",
   "metadata": {},
   "source": [
    "#### Text + Metadata model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8389db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, val loss: inf -> 0.00220, train loss: 0.00268\n",
      "Epoch 1, val loss: 0.00220 -> 0.00214, train loss: 0.00242\n",
      "Epoch 2, val loss: 0.00214 -> 0.00213, train loss: 0.00236\n"
     ]
    }
   ],
   "source": [
    "##### Text + metadata Transformer Model\n",
    "encoder = AutoModel.from_pretrained('roberta-base', return_dict=True)\n",
    "\n",
    "# Freeze encoder parameters to avoid CUDA out of memory.\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = UsefulScoreRegressorAllFeat(encoder, num_meta_feats=len(nonTextCols))\n",
    "model = model.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_text_meta_model(num_epochs=3, model=model, optimizer=optimizer,\n",
    "                      train_loader=train_loader, val_loader=val_loader,\n",
    "                      criterion=criterion, save_path='../models/RoBERTa_Frozen_TextMeta_Clip.pt', clip=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9270eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check how the model is performing across each metadata feature group\n",
    "#### Especially important to look at performance by age of review, to see if performance is good for young reviews\n",
    "#### In practice, the newly posted reviews would be the ones that the model would help prioritize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b79ac4",
   "metadata": {},
   "source": [
    "#### Train Models (Free Transformer Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b7596",
   "metadata": {},
   "source": [
    "#### Text-only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443723d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.00 GiB total capacity; 9.52 GiB already allocated; 8.50 MiB free; 9.61 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1d8f7c007216>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m train_text_model(num_epochs=3, model=model, optimizer=optimizer,\n\u001b[0;32m     10\u001b[0m                  \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                  criterion=criterion, save_path='../models/RoBERTa_Free_TextOnly_Clip.pt', clip=1.0)\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\GT\\MSCS\\Fall_2021\\CS7650_NLP\\Final_Project\\DrugReviews\\GitHub\\NLP_Final_Project\\utils\\training.py\u001b[0m in \u001b[0;36mtrain_text_model\u001b[1;34m(num_epochs, model, optimizer, train_loader, val_loader, criterion, save_path, clip)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# get outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mtot_val_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GT\\MSCS\\Fall_2021\\CS7650_NLP\\Final_Project\\DrugReviews\\GitHub\\NLP_Final_Project\\models\\transformer_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tokens, attention_mask)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_lin1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         )\n\u001b[0;32m    837\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    488\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                 )\n\u001b[0;32m    492\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m         )\n\u001b[0;32m    414\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m         )\n\u001b[0;32m    351\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.00 GiB total capacity; 9.52 GiB already allocated; 8.50 MiB free; 9.61 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "##### Text-only Transformer Model\n",
    "encoder = AutoModel.from_pretrained('roberta-base', return_dict=True)\n",
    "\n",
    "model = UsefulScoreRegressorTextOnly(encoder)\n",
    "model = model.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_text_model(num_epochs=3, model=model, optimizer=optimizer,\n",
    "                 train_loader=train_loader, val_loader=val_loader,\n",
    "                 criterion=criterion, save_path='../models/RoBERTa_Free_TextOnly_Clip.pt', clip=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa77e4",
   "metadata": {},
   "source": [
    "#### Text + Metadata Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ab24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Text + metadata Transformer Model\n",
    "encoder = AutoModel.from_pretrained('roberta-base', return_dict=True)\n",
    "\n",
    "model = UsefulScoreRegressorAllFeat(encoder, num_meta_feats=len(nonTextCols))\n",
    "model = model.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_text_meta_model(num_epochs=3, model=model, optimizer=optimizer,\n",
    "                      train_loader=train_loader, val_loader=val_loader,\n",
    "                      criterion=criterion, save_path='../models/RoBERTa_Free_TextMeta_Clip.pt', clip=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b04d4e",
   "metadata": {},
   "source": [
    "#### Train Meta-only Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20d7f6",
   "metadata": {},
   "source": [
    "#### With Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320d550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, val loss: inf -> 0.00221, train loss: 0.00271\n",
      "Epoch 1, val loss: 0.00221 -> 0.00218, train loss: 0.00231\n",
      "Epoch 2, val loss: 0.00218 -> 0.00218, train loss: 0.00227\n"
     ]
    }
   ],
   "source": [
    "model = UsefulScoreRegressorMetaOnly(num_meta_feats=len(nonTextCols))\n",
    "model = model.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_meta_model(num_epochs=3, model=model, optimizer=optimizer,\n",
    "                 train_loader=train_loader, val_loader=val_loader,\n",
    "                 criterion=criterion, save_path='../models/RoBERTa_MetaOnly_Clip.pt', clip=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945136e1",
   "metadata": {},
   "source": [
    "#### Without Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41e44b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, val loss: inf -> 0.00224, train loss: 0.00244\n",
      "Epoch 1, val loss: 0.00224 -> 0.00220, train loss: 0.00230\n",
      "Epoch 2, val loss: 0.00220, train loss: 0.00228\n"
     ]
    }
   ],
   "source": [
    "model = UsefulScoreRegressorMetaOnly(num_meta_feats=len(nonTextCols))\n",
    "model = model.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_meta_model(num_epochs=3, model=model, optimizer=optimizer,\n",
    "                 train_loader=train_loader, val_loader=val_loader,\n",
    "                 criterion=criterion, save_path='../models/RoBERTa_MetaOnly_NoClip.pt', clip=1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7900619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
